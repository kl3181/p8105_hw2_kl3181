---
title: "p8105_hw2_kl3181"
author: Kelley Lou
output: github_document
---

```{r setup}
library(tidyverse)
library(readxl)
```

## Problem 1
Read in Mr. Trashweel data set and clean.
```{r}
trashwheel_df = read_xlsx(
  "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
      sheet = "Mr. Trash Wheel",
      range = cell_cols("A:N")) %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>% 
  mutate(
      sports_balls = round(sports_balls),
      sports_balls = as.integer(sports_balls)
  )
```

Read in precipitation data for 2018 and 2017. 
```{r}
precip_2018 = read_excel(
  "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
      sheet = "2018 Precipitation", 
      skip = 1
      ) %>%
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2018) %>%
  relocate(year)

precip_2017 = read_excel(
  "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
      sheet = "2017 Precipitation",
      skip = 1
      ) %>%
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2017) %>%
  relocate(year)
```

Now combine annual precipitation dataframes. This can be done with a "helper" tibble that contains pairs of numeric and character ways of representing month, and then merge with precipitation dataset. 
```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2018, precip_2017)

precip_df = 
  left_join(precip_df, month_df, by = "month")
```

This dataset contains information from the Mr.Trashweel trash collector in Baltimore, Maryland. As trash enters the inner harbor, the trashweel collects that trash, and then stores it in a dumpster. The dataset contains information on year, month, and trash collected, including some specific kinds of trash. There are a total of `r nrow(trashwheel_df)` rows in our final dataset. Additional data sheets include month precipitation data. There are variables including `r colnames(trashwheel_df)` and has data from `r min(pull(trashwheel_df, year))` to `r max(pull(trashwheel_df, year))`. The median number of sports balls in 2017 was `r trashwheel_df %>% filter(year == 2017) %>% pull(sports_balls) %>% median()`. The total precipitation in 2017 was `r precip_df %>% filter(year == 2017) %>% pull(total) %>% sum()` inches. In 2018, it was `r precip_df %>% filter(year == 2018) %>% pull(total) %>% sum()` inches.

## Problem 2
#####Read in NYC transit dataset and clean.
```{r}
transit_df =  read_csv(
  "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
    janitor::clean_names() %>%
    select(line:entry, vending, ada) %>% 
    mutate(
      entry = recode(entry, "YES" = "TRUE", "NO" = "FALSE"
    ), 
)
```
This dataset focuses on NYC transit data, with information related to each entrance and exit for each subway station in NYC. After cleaning the dataset, the dataset contains variables `r colnames(transit_df)`. The clean_names function was utilized in the janitor package. The entry variable was changed from character to logical. Additionally, there are a total of `r nrow(transit_df)` rows and `r ncol(transit_df)` columns. The data is not tidy because the route number and route name are not distinct variables.

##### Data Related Questions
There are `r count(distinct(transit_df, station_name, line))` distinct stations, `r filter(transit_df, ada == "TRUE") %>% distinct(station_name, line) %>% nrow` stations are ADA compliant and `r (filter(transit_df, vending == "NO" & entry == "TRUE") %>% nrow) / (filter(transit_df, vending == "NO") %>% nrow)*100` percent of entrances/exits without vending allow entrance.

##### Reformat data
```{r}
transit_reformat = 
  transit_df %>% 
  mutate_at(vars(route8:route11), as.character) %>% 
  pivot_longer(
    route1:route11, 
    names_to = "route_num", 
    values_to = "route_name") %>% 
  drop_na(route_name)

A_train = transit_reformat %>% 
  filter(route_name == "A") %>% 
  distinct(line, station_name, .keep_all = TRUE)

ADA_A = A_train %>% 
  filter(ada == TRUE)
```
There are `r nrow(A_train)` distinct stations serving the A train. Among these stations, `r nrow(ADA_A)` are ADA compliant.


## Problem 3
Reading in and cleaning pols-month data.
```{r}
pols_month_df = read_csv(
  "./data/pols-month.csv") %>%
  janitor::clean_names() %>%
  separate(mon, into = c("year", "month", "day"))

month_pols =
  tibble(
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    month_name = month.name
  ) %>% view

pols_tidy_df = left_join(pols_month_df, month_pols, by = "month") %>%
  relocate(month_name) %>%
  mutate(
    president = recode(prez_dem,`1` = "dem", `0` = "gop")) %>%
  select(-month, -prez_dem, -prez_gop, -day) 
```

Reading in and cleaning snp data.
```{r}
snp_df = read_csv(
  "./data/snp.csv") %>%
  janitor::clean_names() %>%
  separate(date, into = c("month", "day", "year"))

month_snp =
  tibble(
    month = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"),
    month_name = month.name
  )

snp_tidy_df = left_join(snp_df, month_snp, by = "month") %>%
  arrange(year, month) %>%
  relocate(year, month_name) %>%
  select(-month, -day) 
```

Reading in and cleaning unemployment data.
```{r}
unemployment_df = read_csv(
  "./data/unemployment.csv") %>%
  janitor::clean_names() %>%
  rename(January = jan, February = feb, March = mar, April = apr, May = may, June = jun, July = jul, August = aug, September = sep, October = oct, November = nov, December = dec) %>% 
  pivot_longer(
    January:December,
    names_to = "month_name",
    values_to = "unemployment"
  ) %>% 
  mutate(
    year = as.character(year)) 
```

Join the datasets.
First, merge snp into pols. Then, merge in unemployment.
```{r}
snp_pols_df = left_join(pols_tidy_df, snp_tidy_df, by = c("year" = "year", "month_name" = "month_name"))

final_df = left_join(snp_pols_df, unemployment_df, by = c("year" = "year", "month_name" = "month_name"))
``` 

The initial pols dataset contains 822 observations of 9 variables related to the number of national politicians who are democratic or republican at any time as well as the date and indication of whether the president was democratic or republican. To tidy the resulting dataset, I separated the date into three new variables: month, year and day and then removing day in order to merge with the other datasets. Then, I converted the months into the full names and created a new president variable, allowing us to see the party of the president easier. The tidied dataset also has 822 rows and 9 columns, with a date range from 1947-2015. 

The initial snp dataset contains 787 observations of 2 variables (date and close) related to the S&P, which is often used as a representative measure of the stock market as a whole. To tidy the dataset, I separated the date into month, year and day, removing day. Then, the full month name was appied to keep consistency. The tidied dataset has 787 rows and 3 columns, with a date range from 1950-2015. 

The initial unemployment dataset contains 68 observations of 13 variables, with each variable indicating the percentage of unemployment for each month as well as the year. To tidy the data and allow for future merging, I transformed the months from columns into rows and then changed the months to allow for consistency. The tidied dataset has 816 rows and 3 variables (year, month and unemployment) with a date range from 1948-2015.

The final dataset merges all three of the other datasets: pols, snp, and unemployment by year and month. This larger dataset contains 822 rows of data and 11 columns. These variables provide information on politics, the stock market as well as unemployment for different time periods ranging from 1947-2015. 
